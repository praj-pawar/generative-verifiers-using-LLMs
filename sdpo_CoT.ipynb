{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOGX+NQw5atk8amXnZMxZlf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praj-pawar/generative-verifiers-using-LLMs/blob/main/sdpo_CoT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU bitsandbytes datasets accelerate loralib peft transformers trl"
      ],
      "metadata": {
        "id": "9DJ_Pj2ljpYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from trl import DPOTrainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\"\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class StepwiseDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        prompt = item['prompt']\n",
        "        steps = item['steps']\n",
        "\n",
        "        chosen_steps = [step['text'] for step in steps]\n",
        "        rejected_steps = [step['rejected'] for step in steps]\n",
        "        scores = [step['score'] for step in steps]\n",
        "\n",
        "        chosen_text = prompt + \" \" + \" \".join(chosen_steps)\n",
        "        rejected_text = prompt + \" \" + \" \".join(rejected_steps)\n",
        "\n",
        "        chosen_inputs = self.tokenizer(chosen_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        rejected_inputs = self.tokenizer(rejected_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            'prompt': prompt,\n",
        "            'chosen': chosen_text,\n",
        "            'rejected': rejected_text,\n",
        "            'chosen_input_ids': chosen_inputs['input_ids'].squeeze(),\n",
        "            'chosen_attention_mask': chosen_inputs['attention_mask'].squeeze(),\n",
        "            'rejected_input_ids': rejected_inputs['input_ids'].squeeze(),\n",
        "            'rejected_attention_mask': rejected_inputs['attention_mask'].squeeze(),\n",
        "            'scores': torch.tensor(scores, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class StepwiseDPOTrainer(DPOTrainer):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self.reward_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")  # Replace with an actual accessible model\n",
        "        self.reward_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")  # Replace with an actual accessible model\n",
        "\n",
        "    def get_llm_score(self, prompt, step):\n",
        "        # Construct input for chain-of-thought reasoning\n",
        "        cot_prompt = f\"\"\"\n",
        "        Problem: {prompt}\n",
        "        Step: {step}\n",
        "\n",
        "        Let's evaluate this step:\n",
        "        1. Is the step logically correct?\n",
        "        2. Does it contribute to solving the problem?\n",
        "        3. Is it clear and well-explained?\n",
        "\n",
        "        Reasoning:\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = self.reward_tokenizer(cot_prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.reward_model.generate(**inputs, max_new_tokens=200, num_return_sequences=1)\n",
        "\n",
        "        reasoning = self.reward_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # We're not using this score, but keeping the function signature consistent\n",
        "        score = 0\n",
        "\n",
        "        return score, reasoning\n",
        "\n",
        "    def compute_loss(self, model, inputs):\n",
        "        chosen_input_ids = inputs['chosen_input_ids']\n",
        "        chosen_attention_mask = inputs['chosen_attention_mask']\n",
        "        rejected_input_ids = inputs['rejected_input_ids']\n",
        "        rejected_attention_mask = inputs['rejected_attention_mask']\n",
        "        scores = inputs['scores']\n",
        "\n",
        "        chosen_logits = model(input_ids=chosen_input_ids, attention_mask=chosen_attention_mask).logits\n",
        "        rejected_logits = model(input_ids=rejected_input_ids, attention_mask=rejected_attention_mask).logits\n",
        "\n",
        "        chosen_log_probs = torch.log_softmax(chosen_logits, dim=-1)\n",
        "        rejected_log_probs = torch.log_softmax(rejected_logits, dim=-1)\n",
        "\n",
        "        loss = -torch.mean(scores * (chosen_log_probs.sum(dim=-1) - rejected_log_probs.sum(dim=-1)))\n",
        "        return {'loss': loss}\n",
        "\n",
        "def process_data(sample, get_llm_score):\n",
        "    problem = sample['question']['problem']\n",
        "    steps = sample['label']['steps']\n",
        "\n",
        "    processed_steps = []\n",
        "    for step in steps:\n",
        "        if step['chosen_completion'] is not None:\n",
        "            chosen_step = step['completions'][step['chosen_completion']]\n",
        "            rejected_step = step['completions'][(step['chosen_completion'] + 1) % len(step['completions'])]\n",
        "        elif step['human_completion']:\n",
        "            chosen_step = step['human_completion']\n",
        "            rejected_step = step['completions'][0]  # Assuming at least one completion exists\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # Use the rating from the dataset as the score\n",
        "        score = chosen_step['rating']\n",
        "\n",
        "        # Get the chain of thought reasoning\n",
        "        _, reasoning = get_llm_score(problem, chosen_step['text'])\n",
        "\n",
        "        processed_steps.append({\n",
        "            'text': chosen_step['text'],\n",
        "            'rejected': rejected_step['text'],\n",
        "            'score': score,\n",
        "            'reasoning': reasoning\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'prompt': problem,\n",
        "        'steps': processed_steps\n",
        "    }\n",
        "\n",
        "def load_data(file_path, get_llm_score):\n",
        "    dataset = load_dataset('json', data_files=file_path)\n",
        "    return dataset['train'].map(\n",
        "        lambda x: process_data(x, get_llm_score),\n",
        "        remove_columns=dataset['train'].column_names\n",
        "    )\n",
        "\n",
        "def main():\n",
        "    logger.info(\"Starting Stepwise DPO training with LLM reward model\")\n",
        "\n",
        "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "        ref_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    ref_model.to(device)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=1000,\n",
        "        load_best_model_at_end=True,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    trainer = StepwiseDPOTrainer(\n",
        "        model=model,\n",
        "        ref_model=ref_model,\n",
        "        args=training_args,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    train_data = load_data('phase2_train.jsonl', trainer.get_llm_score)\n",
        "    eval_data = load_data('phase2_test.jsonl', trainer.get_llm_score)\n",
        "\n",
        "    train_dataset = StepwiseDataset(train_data, tokenizer, max_length=512)\n",
        "    eval_dataset = StepwiseDataset(eval_data, tokenizer, max_length=512)\n",
        "\n",
        "    trainer.train_dataset = train_dataset\n",
        "    trainer.eval_dataset = eval_dataset\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(\"./final_model\")\n",
        "\n",
        "    logger.info(\"Stepwise DPO training with LLM reward model completed successfully\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "l2i_BHMcjyTT",
        "outputId": "edf0ac88-4843-4fe9-8043-f6c5f048b58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from trl import DPOTrainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\"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\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class StepwiseDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        prompt = item['prompt']\n",
        "        steps = item['steps']\n",
        "\n",
        "        chosen_steps = [step['text'] for step in steps]\n",
        "        rejected_steps = [step['rejected'] for step in steps]\n",
        "        scores = [step['score'] for step in steps]\n",
        "\n",
        "        chosen_text = prompt + \" \" + \" \".join(chosen_steps)\n",
        "        rejected_text = prompt + \" \" + \" \".join(rejected_steps)\n",
        "\n",
        "        chosen_inputs = self.tokenizer(chosen_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        rejected_inputs = self.tokenizer(rejected_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            'prompt': prompt,\n",
        "            'chosen': chosen_text,\n",
        "            'rejected': rejected_text,\n",
        "            'chosen_input_ids': chosen_inputs['input_ids'].squeeze(),\n",
        "            'chosen_attention_mask': chosen_inputs['attention_mask'].squeeze(),\n",
        "            'rejected_input_ids': rejected_inputs['input_ids'].squeeze(),\n",
        "            'rejected_attention_mask': rejected_inputs['attention_mask'].squeeze(),\n",
        "            'scores': torch.tensor(scores, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class StepwiseDPOTrainer(DPOTrainer):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        self.reward_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")  # Replace with an actual accessible model\n",
        "        self.reward_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")  # Replace with an actual accessible model\n",
        "\n",
        "    def get_llm_reasoning(self, prompt, step):\n",
        "        cot_prompt = f\"\"\"\n",
        "        Problem: {prompt}\n",
        "        Step: {step}\n",
        "\n",
        "        Let's evaluate this step:\n",
        "        1. Is the step logically correct?\n",
        "        2. Does it contribute to solving the problem?\n",
        "        3. Is it clear and well-explained?\n",
        "\n",
        "        Reasoning:\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = self.reward_tokenizer(cot_prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.reward_model.generate(**inputs, max_new_tokens=200, num_return_sequences=1)\n",
        "\n",
        "        reasoning = self.reward_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        return reasoning\n",
        "\n",
        "    def compute_loss(self, model, inputs):\n",
        "        chosen_input_ids = inputs['chosen_input_ids']\n",
        "        chosen_attention_mask = inputs['chosen_attention_mask']\n",
        "        rejected_input_ids = inputs['rejected_input_ids']\n",
        "        rejected_attention_mask = inputs['rejected_attention_mask']\n",
        "        scores = inputs['scores']\n",
        "\n",
        "        chosen_logits = model(input_ids=chosen_input_ids, attention_mask=chosen_attention_mask).logits\n",
        "        rejected_logits = model(input_ids=rejected_input_ids, attention_mask=rejected_attention_mask).logits\n",
        "\n",
        "        chosen_log_probs = torch.log_softmax(chosen_logits, dim=-1)\n",
        "        rejected_log_probs = torch.log_softmax(rejected_logits, dim=-1)\n",
        "\n",
        "        loss = -torch.mean(scores * (chosen_log_probs.sum(dim=-1) - rejected_log_probs.sum(dim=-1)))\n",
        "        return {'loss': loss}\n",
        "\n",
        "def process_data(sample, get_llm_reasoning):\n",
        "    problem = sample['question']['problem']\n",
        "    steps = sample['label']['steps']\n",
        "\n",
        "    processed_steps = []\n",
        "    for step in steps:\n",
        "        if step['chosen_completion'] is not None:\n",
        "            chosen_step = step['completions'][step['chosen_completion']]\n",
        "            rejected_step = step['completions'][(step['chosen_completion'] + 1) % len(step['completions'])]\n",
        "        elif step['human_completion']:\n",
        "            chosen_step = step['human_completion']\n",
        "            rejected_step = step['completions'][0]  # Assuming at least one completion exists\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # Use the rating from the dataset as the score\n",
        "        score = chosen_step['rating']\n",
        "\n",
        "        # Get the chain of thought reasoning\n",
        "        reasoning = get_llm_reasoning(problem, chosen_step['text'])\n",
        "\n",
        "        processed_steps.append({\n",
        "            'text': chosen_step['text'],\n",
        "            'rejected': rejected_step['text'],\n",
        "            'score': score,\n",
        "            'reasoning': reasoning\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'prompt': problem,\n",
        "        'steps': processed_steps\n",
        "    }\n",
        "\n",
        "def load_data(file_path, get_llm_reasoning):\n",
        "    dataset = load_dataset('json', data_files=file_path)\n",
        "    return dataset['train'].map(\n",
        "        lambda x: process_data(x, get_llm_reasoning),\n",
        "        remove_columns=dataset['train'].column_names\n",
        "    )\n",
        "\n",
        "def main():\n",
        "    logger.info(\"Starting Stepwise DPO training with LLM reward model\")\n",
        "\n",
        "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "        ref_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    ref_model.to(device)\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=1000,\n",
        "        load_best_model_at_end=True,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    trainer = StepwiseDPOTrainer(\n",
        "        model,\n",
        "        ref_model,\n",
        "        args=training_args,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    train_data = load_data('phase2_train.jsonl', trainer.get_llm_reasoning)\n",
        "    eval_data = load_data('phase2_test.jsonl', trainer.get_llm_reasoning)\n",
        "\n",
        "    train_dataset = StepwiseDataset(train_data, tokenizer, max_length=512)\n",
        "    eval_dataset = StepwiseDataset(eval_data, tokenizer, max_length=512)\n",
        "\n",
        "    trainer.train_dataset = train_dataset\n",
        "    trainer.eval_dataset = eval_dataset\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(\"./final_model\")\n",
        "\n",
        "    logger.info(\"Stepwise DPO training with LLM reward model completed successfully\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "ejWaKEfk5093",
        "outputId": "77663633-0c99-4a35-c311-d352abc6dc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import trl.trainer.dpo_trainer because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'isin_mps_friendly' from 'transformers.pytorch_utils' (/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"peft\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_peft_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPEFT_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"jinja\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_jinja_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJINJA_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m     ]\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCausalLMOutputWithPast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2SeqLMOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misin_mps_friendly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtensionsTrie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'isin_mps_friendly' from 'transformers.pytorch_utils' (/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"peft\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_peft_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPEFT_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"jinja\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_jinja_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJINJA_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m     ]\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftAdapterMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepspeed_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_deepspeed_zero3_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_tokenizers_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOKENIZERS_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_torch_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPYTORCH_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTORCHVISION_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1604\u001b[0m     ]\n\u001b[0;32m-> 1605\u001b[0;31m )\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'isin_mps_friendly' from 'transformers.pytorch_utils' (/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1592\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_tokenizers_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOKENIZERS_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_torch_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPYTORCH_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTORCHVISION_IMPORT_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1604\u001b[0m     ]\n\u001b[0;32m-> 1605\u001b[0;31m )\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'isin_mps_friendly' from 'transformers.pytorch_utils' (/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a6afba29c978>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDPOTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module {self.__name__} has no attribute {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import trl.trainer.dpo_trainer because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\ncannot import name 'isin_mps_friendly' from 'transformers.pytorch_utils' (/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fgSEsgzqyMqt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}